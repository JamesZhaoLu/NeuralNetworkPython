{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Hyper Parameters\n",
    "\n",
    "A lot of hyper parameters to select such as \n",
    "* number of layers\n",
    "* activiation function of each layer\n",
    "* number of neuros in each layer\n",
    "* learning rate\n",
    "* etc\n",
    "It is helpful to know under which circumstances to use what hyper parameters\n",
    "\n",
    "### Number of Hidden Layers\n",
    "Most problems you can begin with one hidden layer (or two). In fact in some NLP models, you can only use one hidden layer and that's good enough if you have enough neurons\n",
    "\n",
    "For more complex problems gradually ramp up the number of hidden layers until you start over fitting the data. Hidden Layers often model the real world hiearchy and relationships. As such deeper hidden layers will often model lower data hierarchy. As such, similar NN can even resue weights and biases of certain lower layers to kick start learning - called transfer learning\n",
    "\n",
    "### Number of Neurons Per Hidden Layer\n",
    "Number of neurons for input and output layers is determined by the shape of your input and output data\n",
    "\n",
    "Common practice is to make the number of neurons like a pyramid. I.e. Most neurons in the first hidden layer, the less in second and so on. E.g. 300 1st, 200 2nd, 100 3rd. The rational is multiple lower level features can coalesce into multiple higher level features. BUT this practice is now largely abandoneded\n",
    "\n",
    "Nowadays using the same number of neurons in each layer is basically the same performance as the pyramid structure (or even better). And is easier since you only have 1 consistent number of neurons per layer. E.g. 150 neurons for all 3 hidden layers. \n",
    "\n",
    "If really complex, can try increasing the number of neurons per layer until you over fit. Or just use early stopping to prevent over fitting\n",
    "\n",
    "In general increasing the number of layers (depth) has better affects on accuracy than increasing the number of neurons.\n",
    "\n",
    "### Learning Rate\n",
    "The most important Hyper parameter\n",
    "\n",
    "In general the optimal learning rate is about half of the maximum leaning rate - i.e. maximum learning rate is rate where the algorithm diverges\n",
    "\n",
    "So the process is start with large learning rate that makes algorithm diverge. Than divide LR by 3 then try again. Then repeat this until algorithm no longer diverges. Then you're pretty close to optimal learning rate.\n",
    "\n",
    "### Batch Size\n",
    "Try to keep it under 32 to keep each iteration really fase\n",
    "\n",
    "But try to keep it over 20 to take advantage of hardware and software optimizations with stuff like Matrix Multiplication etc.\n",
    "\n",
    "Therefor\n",
    "\n",
    "### Epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
